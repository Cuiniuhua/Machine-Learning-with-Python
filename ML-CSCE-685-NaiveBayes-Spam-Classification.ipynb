{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "            'Enron-6': [\n",
    "                ('./ham-spam/data/enron6/ham', 'ham'),\n",
    "                ('./ham-spam/data/enron6/spam', 'spam')\n",
    "            ]\n",
    "}\n",
    "\n",
    "key = 'Enron-6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEWLINE = '\\n'\n",
    "\n",
    "# Read the content of a file\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for dir_name in dir_names:\n",
    "            pass\n",
    "        for file_name in file_names:        \n",
    "            file_path = os.path.join(path, file_name)\n",
    "            lines = []\n",
    "            f = open(file_path, mode='r', encoding='latin-1')\n",
    "            for line in f:\n",
    "                lines.append(line)\n",
    "            f.close()\n",
    "            content = NEWLINE.join(lines)\n",
    "            yield file_path, content\n",
    "\n",
    "# create a DataFrame by reading each files in the given directory\n",
    "def create_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_path, content in read_files(path):\n",
    "        rows.append({'e-mail-text':content, 'class':classification})\n",
    "        index.append(file_path)\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re, nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def lemmatize_tokens(tokens, lemmatizer):\n",
    "    lemmatized = []\n",
    "    for item in tokens:\n",
    "        lemmatized.append(lemmatizer.lemmatize(item))\n",
    "    return lemmatized\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # stem\n",
    "    #stems = stem_tokens(tokens, stemmer)\n",
    "    # lemmatize\n",
    "    lemmas = lemmatize_tokens(tokens, lemmatizer)\n",
    "    #return stems\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['ham', 'spam']\n",
    "def display_confusion_matrix(cm, cmap):\n",
    "    plt.figure()\n",
    "    #figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.matshow(cm, cmap=cmap, interpolation='nearest')\n",
    "    plt.title('confusion matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks,labels)\n",
    "    plt.ylabel('expected label')\n",
    "    plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "pipelines_tf = [\n",
    "                Pipeline([\n",
    "                    ('MultinomialNB TF', MultinomialNB(alpha=0.01))   # testing accuracy of Multinomial Naive Bayes\n",
    "                ]),\n",
    "                Pipeline([\n",
    "                   ('BernoulliNB TF', BernoulliNB(alpha=0.01))       # testing accuracy of Bernoulli Naive Bayes\n",
    "                ]),\n",
    "                Pipeline([\n",
    "                    ('GaussianNB TF', GaussianNB())                   # testing accuracy of Gaussian Naive Bayes\n",
    "                ])                       \n",
    "            ]\n",
    "pipelines_tfidf = [\n",
    "                Pipeline([\n",
    "                    ('MultinomialNB TFIDF', MultinomialNB(alpha=0.01))   # testing accuracy of Multinomial Naive Bayes\n",
    "                ]),\n",
    "                Pipeline([\n",
    "                    ('GaussianNB TFIDF', GaussianNB())                   # testing accuracy of Gaussian Naive Bayes\n",
    "                ])       \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def benchmark_clf(clf, X_train, y_train, X_test):\n",
    "    clf.fit(X_train.toarray(), y_train)\n",
    "    \n",
    "    # Predict the class and probability on the test data\n",
    "    y_pred = clf.predict(X_test.toarray())\n",
    "    y_pred_prob = clf.predict_proba(X_test.toarray())[:, 1]\n",
    "    \n",
    "    return y_pred, y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# create an initial DataFrame\n",
    "enron_df = DataFrame({'e-mail-text':[], 'class':[]})\n",
    "for path, classification in DATASETS[key]:\n",
    "    enron_df = enron_df.append(create_data_frame(path, classification))\n",
    "\n",
    "# shuffle entries so that ham and spam are not consecutively stored\n",
    "#enron_df.reindex(np.random.permutation(enron_df.index))\n",
    "#print(enron_df.head(10))\n",
    "\n",
    "# stratified sampling of training and test data based on the distribution of ham and spam\n",
    "X_train, X_test, y_train, y_test = train_test_split(enron_df['e-mail-text'], \n",
    "                                                    enron_df['class'], random_state=0, stratify=enron_df['class'])\n",
    "\n",
    "# create the term frequency vectors\n",
    "count_vect = CountVectorizer(analyzer = 'word', tokenizer=tokenize, lowercase=True, stop_words='english')\n",
    "#count_vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "count_vect.fit(enron_df['e-mail-text'])\n",
    "#print(count_vect.get_feature_names)\n",
    "\n",
    "X_train_tf = count_vect.transform(X_train)\n",
    "print('sparse matrix shape:', X_train_tf.shape)\n",
    "print('number of non-zeros:', X_train_tf.nnz)\n",
    "print('sparsity: %.2f%%' % (100.0 * X_train_tf.nnz / (X_train_tf.shape[0] * X_train_tf.shape[1])))\n",
    "\n",
    "# after the counting, the term weighting and normalization is done with TF-IDF\n",
    "#tfidf_vect = TfidfVectorizer(analyzer=split_into_lemmas)\n",
    "tfidf_vect = TfidfVectorizer(analyzer = 'word', tokenizer=tokenize, lowercase=True, stop_words='english')\n",
    "\n",
    "tfidf_vect.fit(enron_df['e-mail-text'])\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "X_test_tf = count_vect.transform(X_test)\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "\n",
    "results = []\n",
    "for pipeline in pipelines_tf:\n",
    "    y_pred, y_pred_prob = benchmark_clf(pipeline, X_train_tf, y_train, X_test_tf)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob, pos_label='spam')\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    results.append((pipeline.steps[-1][0], y_test, y_pred, y_pred_prob, fpr, tpr, roc_auc))\n",
    "    \n",
    "for pipeline in pipelines_tfidf:\n",
    "    y_pred, y_pred_prob = benchmark_clf(pipeline, X_train_tf, y_train, X_test_tf)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob, pos_label='spam')\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    results.append((pipeline.steps[-1][0], y_test, y_pred, y_pred_prob, fpr, tpr, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='NULL Accuracy')\n",
    "\n",
    "for clf, y_test, y_pred, y_pred_prob, fpr, tpr, roc_auc in results:\n",
    "    plt.plot(fpr, tpr, lw=1, label='%s (AUC = %0.6f)' % (clf, roc_auc))\n",
    "\n",
    "plt.title('ROC Spam Classifier: %s - %s attributes' %(key, len(count_vect.get_feature_names())))\n",
    "plt.xlabel('1 - specificity (1 - ham recall)')\n",
    "plt.ylabel('sensitivity (spam recall)')\n",
    "plt.legend(loc=\"lower right\", fontsize = 'medium', labelspacing = 0.25)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "labels = ['ham', 'spam']\n",
    "for clf, y_test, y_pred, y_pred_prob, fpr, tpr, roc_auc in results:\n",
    "    confmat = metrics.confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.7)\n",
    "    for i in range(confmat.shape[0]):\n",
    "        for j in range(confmat.shape[1]):\n",
    "            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks,labels)\n",
    "    plt.xlabel('predicted label')\n",
    "    plt.ylabel('true label')\n",
    "    plt.tight_layout()\n",
    "    print('Confusion Matrix: %s' %(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for clf, y_test, y_pred, y_pred_prob, fpr, tpr, roc_auc in results:\n",
    "    print('Metrics for %s:' % clf)\n",
    "    # compute precision and recall, or their combination (harmonic mean) F1:\n",
    "    print(metrics.classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
